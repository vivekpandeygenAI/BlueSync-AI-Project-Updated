import os
import json
from typing import List, Dict, Optional
from google.cloud.sql.connector import Connector, IPTypes
import pg8000
import sqlalchemy
from sqlalchemy import text

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from app.core.config import settings


class VectorDBService:
    """
    Vector database service using SQLAlchemy + Cloud SQL Connector + pgvector.
    Stores and queries document embeddings generated by Gemini embeddings.
    """

    def __init__(self):
        # Cloud SQL Connection Settings
        self.instance_connection_name = "connextion-name"  # Update with your instance connection name
        self.db_user = os.getenv("DB_USER", "postgres")
        self.db_pass = os.getenv("DB_PASS", "pass")
        self.db_name = os.getenv("DB_NAME", "postgres")

        ip_type = IPTypes.PRIVATE if os.environ.get("PRIVATE_IP") else IPTypes.PUBLIC
        self.connector = Connector(refresh_strategy="LAZY")

        def getconn() -> pg8000.dbapi.Connection:
            conn = self.connector.connect(
                self.instance_connection_name,
                "pg8000",
                user=self.db_user,
                password=self.db_pass,
                db=self.db_name,
                ip_type=ip_type,
            )
            return conn

        print("üõ† Creating pooled DB engine...")
        self.engine = sqlalchemy.create_engine(
            "postgresql+pg8000://",
            creator=getconn,
            pool_size=5,
            max_overflow=2,
        )

        # Embedding model
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=settings.GOOGLE_API_KEY,
        )

        # Text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200, length_function=len
        )

        self.table_name = "documents"

    # ----------------------------------------------------------------------
    # Store Document
    # ----------------------------------------------------------------------
    def store_document(self, content: str, metadata: Optional[Dict] = None):
        """Split content into chunks, embed, and store in Postgres."""
        try:
            chunks = self.text_splitter.split_text(content)
            print(f"üìÑ Created {len(chunks)} chunks")

            with self.engine.begin() as conn:
                for chunk in chunks:
                    vector = self.embeddings.embed_query(chunk)
                    vector_str = "[" + ",".join(map(str, vector)) + "]"

                    query = text(f"""
                        INSERT INTO {self.table_name} (content, metadata, embedding)
                        VALUES (:content, :metadata, (:embedding)::vector)
                    """)

                    conn.execute(query, {
                        "content": chunk,
                        "metadata": json.dumps(metadata or {}),
                        "embedding": vector_str,
                    })


            print(f"‚úÖ Stored {len(chunks)} chunks successfully in pgvector.")

        except Exception as e:
            print(f"‚ùå Error storing document: {e}")
            raise

    # ----------------------------------------------------------------------
    # Semantic Search
    # ----------------------------------------------------------------------
    def semantic_search(
    self, query: str, top_k: int = 5, metadata_filter: Optional[Dict] = None
) -> List[Dict]:
        """Perform semantic search using cosine similarity."""
        try:
            query_vector = self.embeddings.embed_query(query)
            query_vector_str = "[" + ",".join(map(str, query_vector)) + "]"

            filter_clause = ""
            params = {"query_vec": query_vector_str, "top_k": top_k}

            if metadata_filter:
                filter_clause = "WHERE metadata @> :metadata"
                params["metadata"] = json.dumps(metadata_filter)

            sql = text(f"""
                SELECT content, metadata, (embedding <=> (:query_vec)::vector) AS distance
                FROM {self.table_name}
                {filter_clause}
                ORDER BY distance ASC
                LIMIT :top_k
            """)

            with self.engine.connect() as conn:
                rows = conn.execute(sql, params).fetchall()

            results = [
                {"text": r.content, "metadata": r.metadata, "distance": r.distance}
                for r in rows
            ]

            print(f"üîç Found {len(results)} similar results.")
            return results

        except Exception as e:
            print(f"‚ùå Error during semantic search: {e}")
            raise



# ‚úÖ Global Singleton Instance
vector_db_service = VectorDBService()
